{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed33cd3e",
   "metadata": {},
   "source": [
    "### CDS NYU\n",
    "### DS-GA 3001 | Reinforcement Learning\n",
    "### Lab 01\n",
    "### February 01, 2023\n",
    "\n",
    "\n",
    "# Introduction to OpenAI Gym (Gymnasium)\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d482f2",
   "metadata": {},
   "source": [
    "## Professor\n",
    "\n",
    "\n",
    "Jeremy Curuksu, PhD -- jeremy.cur@nyu.edu\n",
    "\n",
    "\n",
    "## Section Leader\n",
    "\n",
    "\n",
    "Anudeep Tubati -- at5373@nyu.edu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e2ac1f",
   "metadata": {},
   "source": [
    "## Goal of Today's Lab \n",
    "\n",
    "In this Lab, we will build \"environments\" in Gym to understand the key components involved when working with Gym. We will not implement any RL \"agent\" today so we can first focus on building RL interfaces in Gym, and interacting with Gym environments. \n",
    "\n",
    "In the rest of the course and all future labs, we will explore the many different ways for an RL agent to explore an environment, learn, and exploit its learning to make decisions. For today, let's just get comfortable setting up Gym simulation environments.\n",
    "\n",
    "## Resources\n",
    "\n",
    "* Gym: https://www.gymlibrary.dev/ and its wiki https://github.com/openai/gym/wiki\n",
    "* The original paper from OpenAI when Gym was released in 2016: https://arxiv.org/pdf/1606.01540.pdf\n",
    "* In late 2022, Gym was moved to a new platform called Gymnasium, which is now **the only maintained version of Gym**: https://gymnasium.farama.org/\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b681bc",
   "metadata": {},
   "source": [
    "# 1. Install libraries to set up RL environments in Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5d5982",
   "metadata": {},
   "source": [
    "#### At the minimum, you need to create a virtual environment with Python and OpenAI Gym installed:\n",
    "\n",
    "`conda create --name py39 python=3.9` \n",
    "\n",
    "`pip install gym`\n",
    "\n",
    "`pip install gymnasium`\n",
    "\n",
    "To add the virtual env as kernel in Jupyter Notebook:\n",
    "\n",
    "`conda activate py39`\n",
    "\n",
    "`pip install ipykernel`\n",
    "\n",
    "`python -m ipykernel install --user --name=py39`\n",
    "\n",
    "\n",
    "#### Other libraries will soon be needed as the course progresses:\n",
    "\n",
    "\n",
    "* Install **extended Gym packages** (e.g., Atari games, etc): `pip install gym-all` or `conda install -c conda-forge gym-all`\n",
    "\n",
    "\n",
    "* Install the **Arcade Learning Environment**: `pip install ale-py`\n",
    "\n",
    "\n",
    "* Install **box2d**: \n",
    "`pip install gym-box2d` or `conda install -c conda-forge gym-box2d`\n",
    "\n",
    "\n",
    "* Install **pygame**: `pip install pygame` \n",
    "\n",
    "\n",
    "* Install **tensorflow**:\n",
    "`pip install tensorflow` or `conda install -c conda-forge tensorflow`\n",
    "\n",
    "\n",
    "* Install **keras-rl2**:\n",
    "`pip install keras-rl2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1393120",
   "metadata": {},
   "source": [
    "# 2. Build Reinforcement Learning environments in Gym\n",
    "\n",
    "In this section, we will build Reinforcement Learning environments in Gym to understand the key components involved when working with Gym. For today as RL \"agent\", we will use random action (random behavior), so we can focus just on the overall logic and Python commands when building RL interfaces in Gym and interacting with Gym environments. \n",
    "\n",
    "In the rest of the course and all future labs, we will explore the many different ways for an RL agent to explore an environment, learn, and exploit its learning to make decisions. For today, let's just get comfortable setting up Gym simulation environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b0a26a",
   "metadata": {},
   "source": [
    "## 2.1) Build an Atari RL simulation environment: *Breakout video game*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cddaec",
   "metadata": {},
   "source": [
    "Gym is a toolkit for developing and comparing reinforcement learning (RL) algorithms. It offers pre-built, baseline RL environments within which a developer can build and test RL algorithms. \n",
    "\n",
    "**At the most fundamental level, using the Gym library means 1) selecting an environment, and 2) interacting with it:**\n",
    "\n",
    "1. **Gym offers many different environments to select, from classic control use cases (Pendulum, Cart-Pole, Blackjack, etc) to video games (Atari) and simulated robotics (MuJoCo)**. These use cases were selected by OpenAI in 2016 to represent problems that are tractable using existing (21st century) AI technologies, yet complex enough to showcase the need for human-like intelligence.\n",
    "\n",
    "\n",
    "2. **Gym offers Python functions to interact with the created environment**. Most important ones are:\n",
    "    * `reset()`: Resets the state of the environment to the initial state (i.e., it restarts the game)\n",
    "    * `step(action)`:  Step forward by performing an action on the environment and returning the resulting state and reward after taking that action, a flag indicating if the game is over or not, and some metadata information\n",
    "    \n",
    "The `reset` function returns one value, which is a starting state/observation. \n",
    "\n",
    "The `step` function returns four values, which we will call the ``next_state``, ``reward``,  ``done`` and ``info`` variables.\n",
    "\n",
    "-  ``next_state``: This is the observation that the agent will receive\n",
    "   after taking the action.\n",
    "-  ``reward``: This is the reward that the agent will receive after\n",
    "   taking the action.\n",
    "-  ``done``: This is a boolean variable that indicates whether or\n",
    "   not the environment has terminated.\n",
    "-  ``info``: This is a dictionary that might contain additional\n",
    "   information about the environment.\n",
    "\n",
    "In the Atari environments the ``info`` dictionary has a ``ale.lives`` key that tells us how many lives the\n",
    "agent has left. If the agent has 0 lives, then the episode is over.\n",
    "\n",
    "\n",
    "### Here are the most basic Python commands to implement a Gym environments:\n",
    "\n",
    "A concise doc for the Atari Breakout video game available in Gym can be found here: https://gym.openai.com/envs/Breakout-v0/\n",
    "\n",
    "Note these basic commands are identical for all environments in Gym.\n",
    "\n",
    "\n",
    "**WARNING: Graphical rendering often crashes the Python kernel after completion => If this happens don't worry about it, just restart your kernel.** <br> (click on `Kernel`, then `Restart`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0229ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Breakout-v0\", render_mode=\"human\") # Exact name/version of environments can be found in Gym's doc\n",
    "observation = env.reset()\n",
    "for _ in range(500):\n",
    "    action = env.action_space.sample()  # this is where an actor (RL agent) would be inserted\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "env.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3c14bf",
   "metadata": {},
   "source": [
    "**Warning:** The `render_mode=\"human\"` argument renders the environment graphically in a separate window. It is optional, and not always recommended. It is not a good idea to use it when *training* an agent because rendering slows down training a lot. But when looking at an environment for the first time, or when training is complete, it can of course be useful to graphically vizualize how the agent behaves in the environment. But here is the warning: in Jupyter Notebook the Gym's graphical rendering works well but is likely to crash your kernel (or freeze it) once the simulation is complete. So as a habit, be ready to click on `Interrupt` and `Restart` in the Kernel tab of Jupyter Notebook after you run a simulation rendered graphically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9d7e80",
   "metadata": {},
   "source": [
    "### Play interactively with Atari video games in Gym\n",
    "\n",
    "Gym comes with a handy `utils` function called `play` for Atari games.\n",
    "\n",
    "Start the game by firing the ball with `space`, then move left and right with `a` and `d` on your keyboard.\n",
    "\n",
    "How many points can you score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c9984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.utils.play import play  # Import the play module from gym.utils\n",
    "\n",
    "env = gym.make(\"Breakout-v0\")    # Don't use render_mode='human' because this will open 2 windows when invoking the\n",
    "                                 # method play below, and this redundancy can make your keyboard unresponsive\n",
    "    \n",
    "play(env, zoom=2)                # Details of the 'play' function are here: https://gymnasium.farama.org/api/utils/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01912f2c",
   "metadata": {},
   "source": [
    "### Useful command to interact with a Gym environment \n",
    "\n",
    "Let's look at some frequently used attributes and methods of a Gym environment, please checkout the concise doc here: https://gymnasium.farama.org/api/env/\n",
    "\n",
    "`render_mode` takes one argument which can take two possible values:\n",
    "* \"human\": Graphically render the state of the environment and display it on a laptop screen\n",
    "* \"rgb_array\": Returns an array of pixels containing the state (image) of the current environment\n",
    "\n",
    "`env.action_space` provides information on the action space, in particular `env.action_space.n` returns the number of possible actions\n",
    "\n",
    "`env.observation_space` provides information on the state space, in particular `env.observation_space.n` returns the number of states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13014fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "env = gym.make(\"Breakout-v0\")\n",
    "\n",
    "array = env.render(mode=\"rgb_array\")  # Returns the image as a 2d numpy array\n",
    "\n",
    "print(array[60][50])                  # Print one of the pixels - (row 60, column 50) corresponds to a red block\n",
    "\n",
    "plt.imshow(array);                    # Vizualize the entire array of pixels in Matplotlib "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834065ec",
   "metadata": {},
   "source": [
    "In the case of the Atari video game Breakout, there are 4 possible actions: start the game (fire the ball), move left, move right, or stand still (do nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cebf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {env.action_space.n} possible actions in Breakout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16840d68",
   "metadata": {},
   "source": [
    "We can invoke the method `sample` on `env.action_space` to select a random action amongst the possible actions. Note the \"ale.lives\" info reduces for each live lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time                                                   # Time package used to slown down vizual to human speed\n",
    "env = gym.make(\"Breakout-v0\", render_mode=\"human\")\n",
    "observation = env.reset()                                     # Initialize the environment to start state\n",
    "for _ in range(200):\n",
    "    random_action = env.action_space.sample()                 # Sample a random action\n",
    "    observation, reward, done, info = env.step(random_action) # Step forward by performing action in current state \n",
    "    print(f\"Reward: {reward}, Done: {done}, Info: {info}\")    # Print some of the metadata information\n",
    "    time.sleep(0.01)                                          # Slow down the game so it's easier to follow visually\n",
    "    if done:                                                  # If game is over:\n",
    "        observation = env.reset()                             #    reset the game = initialize the env to start state\n",
    "env.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0169e39f",
   "metadata": {},
   "source": [
    "## 2.2) Build a Moon Lander RL simulation environment\n",
    "\n",
    "This environment is a simple rocket trajectory optimization problem. \n",
    "\n",
    "The goal is to land the rocket safely on the moon, using four possible actions: <br>\n",
    "`0`: Do nothing <br>\n",
    "`1`: Fire left orientation engine <br>\n",
    "`2`: Fire main engine <br>\n",
    "`3`: Fire right orientation engine\n",
    "\n",
    "Details can be found in the Gym/Gymnasium doc: https://gymnasium.farama.org/environments/box2d/lunar_lander/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0c40a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym   # Gymnasium is the new, maintained version of Gym, most Gym env are directly compatible\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  \n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7390c664",
   "metadata": {},
   "source": [
    "\n",
    "## 2.3) Build a Cart-Pole RL simulation environment\n",
    "\n",
    "This environment is a very popular, \"classic\" control use case: a pendulum (\"pole\") is attached by an un-actuated joint to a cart, placed upright on the cart, and the cart can move along a frictionless track. \n",
    "\n",
    "The pendulum is placed upright on the cart and The goal is to balance the pole (i.e., keep it upright within -12 to +12 degrees) by applying forces in the left and right direction on the cart, using two possible actions: <br>\n",
    "`0`: Push cart to the left <br>\n",
    "`1`: Push cart to the right\n",
    "\n",
    "**Challenges**: The episode terminates if the cart x-position gets outside the range [-2.4, 2.4] and/or the pole angle gets outside the range [-12°, 12°]. What makes this problem non-trivial is that the velocity which is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it.\n",
    "\n",
    "\n",
    "Details can be found in the Gym/Gymnasium doc: https://gymnasium.farama.org/environments/classic_control/cart_pole/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0897612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym  \n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "print('Cartpole has {} possible actions (either move left or right)'.format(env.action_space.n))\n",
    "\n",
    "# Let's have a look at the starting state:\n",
    "env.reset()\n",
    "env.render(mode=\"human\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e5582",
   "metadata": {},
   "source": [
    "# 3) Exercices\n",
    "\n",
    "## 3.1) Exercice: Define an action-selection rule (not RL-based) for the Moon Lander \n",
    "\n",
    "**Run a simulation of the lunar lander for 2000 steps, firing the left engine whenever the x coordinate of the lander is < -0.1, firing the right engine whenever the x coordinate of the lander is > 0.1, and resetting its state whenever it lands or crashes**\n",
    "\n",
    "**Hint:** The state is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not. Details can be found in the Gym/Gymnasium doc: https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "\n",
    "Again keep in mind today we do not build any \"RL agent\", none of the above or below material aims at 'learning' decisions, today we are just aim to better understand and interact with Gym environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66fb006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to question 3.1:\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55ae344",
   "metadata": {},
   "source": [
    " ## 3.2) Exercice: Interactively control (non RL-based) the Cart-Pole "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b70063",
   "metadata": {},
   "source": [
    "\n",
    "**Run simulations of the cartpole (as many as you like), and manually push the left and right arrows on your keyboard to try and keep the pole balanced by applying forces in the left and right direction on the cart. While you do so, print the cart velocity and the pole angle in degrees.**\n",
    "\n",
    "**Hint:** The state is an 4-dimensional vector: the cart position, the cart velocity, the pole angle, teh pole angular velocity. Details can be found in the Gym/Gymnasium doc: https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "\n",
    "Can you identify approximate thresholds of angle and velocity at which to start pushing either left or right?\n",
    "\n",
    "Again keep in mind today we do not build any \"RL agent\", none of the above or below material aims at 'learning' decisions, today we are just aim to better understand and interact with Gym environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13bfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to question 3.2:\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e68c38a",
   "metadata": {},
   "source": [
    "**Spoiler alert**: When we develop RL agents in this environment, we will add the angular velocity of the pole in addition to pole angle and cart velocity, so the agent has more information to make smart decisions. An RL agent is able to take much more than one or two features into account, simultaneously. As we will see, RL agents can reach superhuman abilities very easily at the Cart-Pole game (and at many other games...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c6274e",
   "metadata": {},
   "source": [
    "## 3.3) Exercice: Explore other Atari video games!\n",
    "\n",
    "Explore the Gym environment for other Atari 2600 video games by creating an instance with graphical rendering, looping 1000 steps, and sampling a random action at each step\n",
    "\n",
    "Pre-installed video games: https://gymnasium.farama.org/environments/atari/complete_list/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32747372",
   "metadata": {},
   "source": [
    "## **Thank you everyone!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
