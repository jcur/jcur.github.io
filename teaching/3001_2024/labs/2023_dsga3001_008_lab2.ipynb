{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3a6e5fb",
   "metadata": {},
   "source": [
    "### CDS NYU\n",
    "### DS-GA 3001 | Reinforcement Learning\n",
    "### Lab 02\n",
    "### February 08, 2023\n",
    "\n",
    "\n",
    "# Multi-Armed Bandits (MAB)\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac95700",
   "metadata": {},
   "source": [
    "## Professor\n",
    "\n",
    "\n",
    "Jeremy Curuksu, PhD -- jeremy.cur@nyu.edu\n",
    "\n",
    "## Section Leader\n",
    "\n",
    "\n",
    "Anudeep Tubati -- email\n",
    "\n",
    "## Goal of Today's Lab \n",
    "\n",
    "In this Lab, we will implement a special case of RL agent called \"Multi-Armed Bandits\", which explores different possible actions in a single, given state, and learns and exploits its learning to maximize the expected reward on the long term.\n",
    "\n",
    "We will not build Gym environments today, and focus on building an MAB agent learning to make decisions in the simple case of a single-state environment. We will build the entire agent-environment interface from scratch.  We will return to Gym environments in future labs when we develop non-MAB agent learning sequential decisions in multi-state environments.\n",
    "\n",
    "The simple case of MAB deals with the key reinforcement learning challenge of exploration vs. exploitation. The MAB agent needs to explore the outcome of different possible actions to learn, but it also needs to exploit its learning to make optimal decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39fe5cf",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* https://gymnasium.farama.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdebe5b1",
   "metadata": {},
   "source": [
    "# 1. MAB with $\\epsilon$-greedy policy exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a6975",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Consider the following learning problem. You are faced repeatedly with a choice among $k$ different options, or actions. After each choice you receive a reward signal in the form of a numerical value, where the larger value is the better. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.\n",
    "\n",
    "This is the original form of the k-armed bandit problem. This name derives from the colloquial name for a slot machine, the \"one-armed bandit\", because it has the one lever to pull, and it is often rigged to take more money than it pays out over time. The multi-armed bandit extension is to imagine, for instance, that you are faced with multiple slot machines that you can play, but only one at a time. Which machine should you play, i.e., which arm should you pull, at any given time to maximize your total payout.\n",
    "\n",
    "<img alt=\"MultiArmedBandit\" width=\"625\" height=\"269\" src=\"https://github.com/NeuromatchAcademy/course-content/blob/main/tutorials/static/W3D4_Tutorial2_MultiarmedBandit.png?raw=true\">\n",
    "\n",
    "While there are many different levels of sophistication and assumptions in how the rewards are determined, we will consider here the simple scenario where each action results in a reward drawn from a different Gaussian distribution with unknown mean and unit variance. Since each action is associated with a different mean reward, the goal of the agent is to find the action with highest mean. But since the rewards are noisy (the corresponding Gaussians have unit variance), those means cannot be determined from a single observed reward. \n",
    "\n",
    "This problem setting is referred to as the *environment*. We will solve this *optimization problem* with an *agent*, in this case an algorithm that takes in rewards and returns actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be662ade",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Q-value action selection in MAB\n",
    "\n",
    "The first thing our agent needs to be able to do is choose which arm to pull. The strategy for choosing actions based on our expectations is called a *policy* (often denoted $\\pi$). We could have a random policy -- just pick an arm at random each time -- though this doesn't seem likely to be capable of optimizing our reward. We want some intentionality, and to do that we need a way of describing our beliefs about the arms' reward potential. We do this with an action-value function\n",
    "\n",
    "\\begin{equation}\n",
    "q(a) = \\mathbb{E} [r_{t} | a_{t} = a]\n",
    "\\end{equation}\n",
    "\n",
    "where the value $q$ for taking action $a \\in A$ at time $t$ is equal to the expected value of the reward $r_t$ given that we took action $a$ at that time. In practice, this is often represented as an array of values (a *lookup table*), where each action's value is a different element in the array.\n",
    "\n",
    "Great, now that we have a way to describe our beliefs about the values each action should return, let's come up with a policy.\n",
    "\n",
    "An obvious choice would be to take the action with the highest expected value. This is referred to as the *greedy* policy\n",
    "\n",
    "\\begin{equation}\n",
    "a_{t} = \\underset{a}{\\operatorname{argmax}} \\; q_{t} (a)\n",
    "\\end{equation}\n",
    "\n",
    "where our choice action is the one that maximizes the current value function.\n",
    "\n",
    "So far so good, but it can't be this easy. And, in fact, the greedy policy does have a fatal flaw: it easily gets trapped in local maxima. It never explores to see what it hasn't seen before if one option is already better than the others. This leads us to a fundamental challenge in coming up with effective policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5435812d",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### The Exploitation-Exploration dilemma\n",
    "\n",
    "If we never try anything new, if we always stick to the safe bet, we don't know what we are missing. Sometimes we aren't missing much of anything, and regret not sticking with our preferred choice, yet other times we stumble upon something new that was way better than we thought.\n",
    "\n",
    "This is the exploitation-exploration dilemma: do you go with your best choice now, or risk the less certain option with the hope of finding something better. Too much exploration, however, means you may end up with a sub-optimal reward once it's time to stop.\n",
    "\n",
    "In order to avoid getting stuck in local minima while also maximizing reward, effective policies need some way to balance between these two aims.\n",
    "\n",
    "A simple extension to our greedy policy is to add some randomness. For instance, a coin flip -- heads we take the best choice now, tails we pick one at random. This is referred to as the $\\epsilon$-greedy policy:\n",
    "\n",
    "\\begin{equation}\n",
    "P (a_{t} = a) = \n",
    "        \\begin{cases}\n",
    "        1 - \\epsilon + \\epsilon/N    & \\quad \\text{if } a_{t} = \\underset{a}{\\operatorname{argmax}}  \\; q_{t} (a) \\\\\n",
    "        \\epsilon/N        & \\quad \\text{else} \n",
    "        \\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "which is to say that with probability 1 - $\\epsilon$ for $\\epsilon \\in [0,1]$ we select the greedy choice, and otherwise we select an action at random (including the greedy option).\n",
    "\n",
    "Despite its relative simplicity, the epsilon-greedy policy is quite effective, which leads to its general popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a4536",
   "metadata": {},
   "source": [
    "## Define custom plotting functions we will use for this case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c79df62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ebd56d",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets  # interactive display\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "#plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "def plot_choices(q, epsilon, choice_fn, n_steps=1000, rng_seed=1):\n",
    "  np.random.seed(rng_seed)\n",
    "  counts = np.zeros_like(q)\n",
    "  for t in range(n_steps):\n",
    "    action = choice_fn(q, epsilon)\n",
    "    counts[action] += 1\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.bar(range(len(q)), counts/n_steps)\n",
    "  ax.set(ylabel='% chosen', xlabel='action', ylim=(0,1), xticks=range(len(q)))\n",
    "\n",
    "\n",
    "def plot_multi_armed_bandit_results(results):\n",
    "  fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(20, 4))\n",
    "  ax1.plot(results['rewards'])\n",
    "  ax1.set(title=f\"Total Reward: {np.sum(results['rewards']):.2f}\",\n",
    "          xlabel='step', ylabel='reward')\n",
    "  ax2.plot(results['qs'])\n",
    "  ax2.set(xlabel='step', ylabel='value')\n",
    "  ax2.legend(range(len(results['mu'])))\n",
    "  ax3.plot(results['mu'], label='latent')\n",
    "  ax3.plot(results['qs'][-1], label='learned')\n",
    "  ax3.set(xlabel='action', ylabel='value')\n",
    "  ax3.legend()\n",
    "\n",
    "\n",
    "def plot_parameter_performance(labels, fixed, trial_rewards, trial_optimal):\n",
    "  fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 6))\n",
    "\n",
    "  ax1.plot(np.mean(trial_rewards, axis=1).T)\n",
    "  ax1.set(title=f'Average Reward ({fixed})', xlabel='step', ylabel='reward')\n",
    "  ax1.legend(labels)\n",
    "\n",
    "  ax2.plot(np.mean(trial_optimal, axis=1).T)\n",
    "  ax2.set(title=f'Performance ({fixed})', xlabel='step', ylabel='% optimal')\n",
    "  ax2.legend(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b31f43e",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Implement an $\\epsilon$-greedy policy\n",
    "\n",
    "In this exercise you will implement the epsilon-greedy algorithm for deciding which action to take from a set of possible actions given their value function and a probability $\\epsilon$ of simply choosing one at random. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d7c05",
   "metadata": {
    "execution": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(q, epsilon):\n",
    "\n",
    "    \"\"\"\n",
    "      Epsilon-greedy policy: selects the maximum value action with probability\n",
    "      (1-epsilon) and selects randomly with epsilon probability.\n",
    "\n",
    "      Args:\n",
    "        q (ndarray): an array of action values\n",
    "        epsilon (float): probability of selecting an action randomly\n",
    "\n",
    "      Returns:\n",
    "        int: the chosen action\n",
    "    \"\"\"\n",
    "    \n",
    "    # write a boolean expression that determines if we should take the best action\n",
    "    be_greedy = None\n",
    "\n",
    "    if be_greedy:\n",
    "        # write an expression for selecting the best action from the action values\n",
    "        \n",
    "    else:\n",
    "        # write an expression for selecting a random action\n",
    "        \n",
    "    return action\n",
    "\n",
    "# Initialize parameters\n",
    "q = [-2, 5, 0, 1]\n",
    "epsilon = 0.1\n",
    "\n",
    "# Visualize\n",
    "with plt.xkcd():\n",
    "    plot_choices(q, epsilon, epsilon_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84205a48",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "**Here is the output to expect:**\n",
    "\n",
    "<img alt='' align='left' width=827.0 height=540.0 src=figure1.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8bddf2",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This is what we should expect, that the action with the largest value (action 1) is selected about (1-$\\epsilon$) of the time, or 90% for $\\epsilon = 0.1$, and the remaining 10% is split evenly amongst the other options. Use the demo below to explore how changing $\\epsilon$ affects the distribution of selected actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ddeb6",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## [optional] Vizualize different $\\epsilon$ schedules\n",
    "\n",
    "Epsilon is our one parameter for balancing exploitation and exploration.  Given a set of values $q = [-2, 5, 0, 1]$, use the widget below to see how changing $\\epsilon$ influences our selection of the max value 5 (action = 1) vs the others. \n",
    "\n",
    "At the extremes of its range (0 and 1), the $\\epsilon$-greedy policy reproduces two other policies. What are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe5c28",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "@widgets.interact(epsilon=widgets.FloatSlider(0.1, min=0.0, max=1.0))\n",
    "def explore_epilson_values(epsilon=0.1):\n",
    "  q = [-2, 5, 0, 1]\n",
    "  plot_choices(q, epsilon, epsilon_greedy, rng_seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a2a080",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "**Take-away**: When $\\epsilon = 0$, the agent always selects the currently best action, i.e., it becomes greedy. When $\\epsilon = 1$, the agent selects actions randomly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb03e6ce",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Learn Q-values from sampled rewards\n",
    "\n",
    "In this exercise you will implement an action-value update rule. The function will take in the action-value function represented as an array `q`, the action taken, the reward received, and a learning rate, `alpha`. The function will return the updated value for the selected action.\n",
    "\n",
    "### *Refresh on the concepts covered during the lecture*:\n",
    "\n",
    "Now that we have a policy for deciding what to do, how do we learn from our actions?\n",
    "\n",
    "One way to do this is just keep a record of every result we ever got and use the averages for each action. If we have a potentially very long running episode, the computational cost of keeping all these values and recomputing the mean over and over again isn't ideal. Instead we can use a moving average approach:\n",
    "\n",
    "\\begin{equation}\n",
    "q_{t+1}(a) \\leftarrow q_{t}(a) + \\frac{1}{n_t} (r_{t} - q_{t}(a))\n",
    "\\end{equation}\n",
    "\n",
    "where our action-value function $q_t(a)$ is the mean of the rewards seen so far, $n_t$ is the number of actions taken by time $t$, and $r_t$ is the reward just received for taking action $a$.\n",
    "\n",
    "The $\\frac{1}{n_t}$ requires to remember how many of each action was taken, and also it attributes the same weight to every update, so in practice this ratio is often replaced by an arbitrary learning rate $\\alpha$:\n",
    "\n",
    "\\begin{equation}\n",
    "q_{t+1}(a) \\leftarrow q_{t}(a) + \\alpha (r_{t} - q_{t}(a)).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35991ba3",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def update_action_value(q, action, reward, alpha):\n",
    "  \n",
    "    \"\"\" \n",
    "      Compute the updated action value given the learning rate and observed\n",
    "      reward.\n",
    "\n",
    "      Args:\n",
    "        q (ndarray): an array of action values\n",
    "        action (int): the action taken\n",
    "        reward (float): the reward received for taking the action\n",
    "        alpha (float): the learning rate\n",
    "\n",
    "      Returns:\n",
    "        float: the updated value for the selected action\n",
    "    \"\"\"\n",
    "\n",
    "    # Write an expression for the updated action value\n",
    "    value = None\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "q = [-2, 5, 0, 1]\n",
    "action = 2\n",
    "print(f\"Original q({action}) value = {q[action]}\")\n",
    "\n",
    "# Update action\n",
    "q[action] = update_action_value(q, 2, 10, 0.01)\n",
    "print(f\"Updated q({action}) value = {q[action]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4003d",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Simulate a Multi-Armed Bandits environment (from scratch)\n",
    "\n",
    "Now that we have both a policy and a learning rule to update q-values, we can combine these to solve the multi-armed bandit task. Recall that we have some number of arms that give rewards drawn from Gaussian distributions with unknown mean and unit variance, and our goal is to find the arm with the highest mean.\n",
    "\n",
    "**First, let's create the environment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454e735a",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def multi_armed_bandit(n_arms, epsilon, alpha, n_steps):\n",
    "    \n",
    "    \"\"\" \n",
    "      A Gaussian multi-armed bandit using an epsilon-greedy policy. For each\n",
    "      action, rewards are randomly sampled from normal distribution, with a mean\n",
    "      associated with that arm and unit variance.\n",
    "\n",
    "      Args:\n",
    "        n_arms (int): number of arms or actions\n",
    "        epsilon (float): probability of selecting an action randomly\n",
    "        alpha (float): the learning rate\n",
    "        n_steps (int): number of steps to evaluate\n",
    "\n",
    "      Returns:\n",
    "        dict: a dictionary containing the action values, actions, and rewards from\n",
    "        the evaluation along with the true arm parameters mu and the optimality of\n",
    "        the chosen actions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Gaussian bandit parameters\n",
    "    mu = np.random.normal(size=n_arms)\n",
    "\n",
    "    # Evaluation and reporting state\n",
    "    q = np.zeros(n_arms)\n",
    "    qs = np.zeros((n_steps, n_arms))\n",
    "    rewards = np.zeros(n_steps)\n",
    "    actions = np.zeros(n_steps)\n",
    "    optimal = np.zeros(n_steps)\n",
    "\n",
    "    # Run the bandit\n",
    "    for t in range(n_steps):\n",
    "\n",
    "        # Compute rewards for all actions (sampled from a Gaussian with mean mu)\n",
    "        all_rewards = np.random.normal(mu)\n",
    "\n",
    "        # Select an action according to an epsilon-greedy policy\n",
    "        action = None\n",
    "        actions[t] = action\n",
    "\n",
    "        # Observe the reward for the chosen action\n",
    "        reward = all_rewards[action]\n",
    "        rewards[t] = reward\n",
    "\n",
    "        # Was it the best possible choice?\n",
    "        optimal_action = np.argmax(all_rewards)\n",
    "        optimal[t] = action == optimal_action\n",
    "\n",
    "        # Update the q-value for the action selected\n",
    "        q[action] = None\n",
    "        qs[t] = q\n",
    "\n",
    "    results = {\n",
    "        'qs': qs,\n",
    "        'actions': actions,\n",
    "        'rewards': rewards,\n",
    "        'mu': mu,\n",
    "        'optimal': optimal\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bfacd3",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can use our multi-armed bandit method to evaluate how our epsilon-greedy policy and learning rule perform at solving the task. First we will set our environment to have 10 arms and our agent parameters to $\\epsilon=0.1$ and $\\alpha=0.01$. In order to get a good sense of the agent's performance, we will run the episode for 1000 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982fcc2",
   "metadata": {},
   "source": [
    "## Evaluate the learning performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980950e",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# set for reproducibility, comment out / change seed value for different results\n",
    "np.random.seed(1)\n",
    "\n",
    "n_arms = 10\n",
    "epsilon = 0.1\n",
    "alpha = 0.01\n",
    "n_steps = 1000\n",
    "\n",
    "results = multi_armed_bandit(n_arms, epsilon, alpha, n_steps)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 6))\n",
    "ax1.plot(results['rewards'])\n",
    "ax1.set(title=f'Observed Reward ($\\epsilon$={epsilon}, $\\\\alpha$={alpha})',\n",
    "        xlabel='step', ylabel='reward')\n",
    "ax2.plot(results['qs'])\n",
    "ax2.set(title=f'Action Values ($\\epsilon$={epsilon}, $\\\\alpha$={alpha})',\n",
    "        xlabel='step', ylabel='value')\n",
    "ax2.legend(range(n_arms));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2852249f",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Rewards seem all over the place (left figure), but the agent quickly settles in on the first arm (action 0) as the preferred choice of action (right figure).\n",
    "\n",
    "Let's see how well we did at recovering the true means of the Gaussian random variables behind the arms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc7301",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(results['mu'], label='latent')\n",
    "ax.plot(results['qs'][-1], label='learned')\n",
    "ax.set(title=f'$\\epsilon$={epsilon}, $\\\\alpha$={alpha}',\n",
    "       xlabel='action', ylabel='value')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5703a",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So it appears the agent found a very good estimate for action 0, but most of the others are not great. We can see the effect of the local maxima trap at work -- the greedy part of our algorithm locked onto action 0, which is actually the 2nd best choice to action 6. These are the means of Gaussian random variables and the overlap between the two is indded quite high, so even if the agent did explore action 6, it could still draw a sample that has a lower estimated value than the estimated value for action 0.\n",
    "\n",
    "This was just one choice of parameters. Perhaps there is a better combination?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e75d29",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Benchmark different values of MAB parameters ($\\epsilon$ and $\\alpha$)\n",
    "\n",
    "Use the widget below to explore how varying the values of $\\epsilon$ (exploitation-exploration tradeoff), $\\alpha$ (learning rate), and even the number of actions $k$, changes the behavior of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4419277",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "@widgets.interact_manual(k=widgets.IntSlider(10, min=2, max=15),\n",
    "                         epsilon=widgets.FloatSlider(0.1, min=0.0, max=1.0),\n",
    "                         alpha=widgets.FloatLogSlider(0.01, min=-3, max=0))\n",
    "def explore_bandit_parameters(k=10, epsilon=0.1, alpha=0.001):\n",
    "  results = multi_armed_bandit(k, epsilon, alpha, 1000)\n",
    "  plot_multi_armed_bandit_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdaeee7",
   "metadata": {
    "execution": {}
   },
   "source": [
    "While we see how changing the epsilon and alpha values impact the agent's behavior, this doesn't give us a good sense of which combination is optimal. Due to the stochastic nature of both the rewards and the policy, a single trial run isn't sufficient to give us this information. Let's run multiple trials and compare the average performance.\n",
    "\n",
    "### Benchmark different $\\epsilon$ schedules\n",
    "\n",
    "Let us look at different values of $\\epsilon \\in [0.0, 0.1, 0.2]$ to a fixed $\\alpha=0.1$. We will run 200 trials (trying to balance speed with accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02116e4e",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Seed set for reproducibility, comment out or change seed value for different results\n",
    "np.random.seed(1)\n",
    "\n",
    "epsilons = [0.0, 0.1, 0.2]\n",
    "alpha = 0.1\n",
    "n_trials = 200\n",
    "trial_rewards = np.zeros((len(epsilons), n_trials, n_steps))\n",
    "trial_optimal = np.zeros((len(epsilons), n_trials, n_steps))\n",
    "for i, epsilon in enumerate(epsilons):\n",
    "    for n in range(n_trials):\n",
    "        results = multi_armed_bandit(n_arms, epsilon, alpha, n_steps)\n",
    "        trial_rewards[i, n] = results['rewards']\n",
    "        trial_optimal[i, n] = results['optimal']\n",
    "\n",
    "labels = [f'$\\epsilon$={e}' for e in epsilons]\n",
    "fixed = f'$\\\\alpha$={alpha}'\n",
    "plot_parameter_performance(labels, fixed, trial_rewards, trial_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5938e3",
   "metadata": {
    "execution": {}
   },
   "source": [
    "On the left we have plotted the average reward over time, and we see that while $\\epsilon=0$ (the greedy policy) does well initially, $\\epsilon=0.1$ starts to do slightly better in the long run, while $\\epsilon=0.2$ does the worst. \n",
    "\n",
    "Looking on the right, we see the percentage of times the optimal action (the best possible choice at time $t$) was taken, and here again we see a similar pattern of $\\epsilon=0.1$ starting out a bit slower but eventually having a slight edge in the longer run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3939dbc9",
   "metadata": {},
   "source": [
    "### Benchmark different $\\alpha$ learning rates\n",
    "\n",
    "Let us do the same for the learning rates. We will evaluate $\\alpha \\in [0.01, 0.1, 1.0]$ to a fixed $\\epsilon=0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473e1571",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Seed set for reproducibility, comment out or change seed value for different results\n",
    "np.random.seed(1)\n",
    "\n",
    "epsilon = 0.1\n",
    "alphas = [0.01, 0.1, 1.0]\n",
    "n_trials = 200\n",
    "trial_rewards = np.zeros((len(epsilons), n_trials, n_steps))\n",
    "trial_optimal = np.zeros((len(epsilons), n_trials, n_steps))\n",
    "for i, alpha in enumerate(alphas):\n",
    "    for n in range(n_trials):\n",
    "        results = multi_armed_bandit(n_arms, epsilon, alpha, n_steps)\n",
    "        trial_rewards[i, n] = results['rewards']\n",
    "        trial_optimal[i, n] = results['optimal']\n",
    "\n",
    "labels = [f'$\\\\alpha$={a}' for a in alphas]\n",
    "fixed = f'$\\epsilon$={epsilon}'\n",
    "plot_parameter_performance(labels, fixed, trial_rewards, trial_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98465e38",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Again we see a balance is needed to find an effective learning rate. $\\alpha=0.01$ is too weak to quickly incorporate good values, while $\\alpha=1$ is too strong likely resulting in high variance in values due to the Gaussian nature of the rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6da656",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Summary\n",
    "\n",
    "In this lab you implemented both the $\\epsilon$-greedy decision algorithm and a learning rule for solving a multi-armed bandit scenario. You saw how balancing exploitation and exploration in action selection is critical in finding optimal solutions. You also saw how choosing an appropriate $\\epsilon$-schedule and learning rate determines how well an agent can generalize the information it receives from rewards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448ac0ef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. [optional] MAB with Thompson sampling policy exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57372c4a",
   "metadata": {},
   "source": [
    "For this case study, we will simulate 3 bandits with each an underlying probability of winning stored in `p_bandits` (unknown to the agent) . A method `pull()` will simulate the pulling of each arm\" by returning if the pull was a win (1) or not (0) based on the underlying probability.\n",
    "\n",
    "\n",
    "#### Python package versions originally used in this case study:\n",
    "    - Python version  : 3.9.4\n",
    "    - IPython version : 7.23.1\n",
    "    - scipy           : 1.6.3\n",
    "    - matplotlib      : 3.4.2\n",
    "    - seaborn         : 0.11.1\n",
    "    - numpy           : 1.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a140c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the multi-armed bandits\n",
    "nb_bandits = 3  # Number of bandits\n",
    "# True probability of winning for each bandit\n",
    "p_bandits = [0.45, 0.55, 0.60]\n",
    "\n",
    "def pull(i):\n",
    "    \"\"\"Pull arm of bandit with index `i` and return 1 if win, \n",
    "    else return 0.\"\"\"\n",
    "    if np.random.rand() < p_bandits[i]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d0e11",
   "metadata": {},
   "source": [
    "## Thompson sampling and Bayesian interpretation\n",
    "Each pull of a specifc bandit will result in a win with a certain probability. The agent doesn't know what this probability is but can model the unkown probability with some parameters and update these parameters based on observation sampled in real time. Using Bayes rule we can write this as:\n",
    "\n",
    "$$ p_t(\\theta | a) \\, \\propto \\, p(r_t | \\theta, a) \\times p_{t-1}(\\theta | a) $$\n",
    " \n",
    "Where $p_t$ is the posterior distribution of parameter $\\theta$ given $a$ and $p_{t-1}$ is the prior. In this case study the likelihood function $ p(r_t | \\theta, a)$ follows the Bernoulli distribution .\n",
    "\n",
    "In this example we will model the priors as Beta distributions because they are conjugate priors to Bernoulli distributions: if the likelihood function is Bernoulli distributed and the prior distribution is Beta distributed then the posterior will also be Beta distributed, thus after every observation we can use the posterior as the prior for the next time we pull the bandit's arm. Details on this and the entire use case can be found at either of the following URLs. Note this case study is entirely optional, and here only for you to explore more advanced topics around MAB (you will not be asked any homework question on this case study). <br>\n",
    "* https://towardsdatascience.com/thompson-sampling-fc28817eacb8 <br>\n",
    "* https://peterroelants.github.io/posts/multi-armed-bandit-implementation/#Further-readings\n",
    "\n",
    "### Maximize reward / Minimize regret\n",
    "The goal of the muli-armed bandit problem is to maximize reward (minimize regret). There is an exploitation-exploration tradeoff the agent has to make. The more it pulls the arm of its perceived best bandit the more certain it becomes of the probability of that bandit. But other bandits that it hasn't pulled that often might have a lower expected probability, but they also have higher uncertaintly since they were pulled less often. There is a chance that they are actually better than the perceived best bandit.\n",
    "\n",
    "Thompson sampling can be used to overcome this problem. In Thompson sampling the agent will sample the probability  from the prior for each bandit, and pull the bandit with the highest sampled probability. And repeat this step until .\n",
    "\n",
    "We will start with the prior , which corresponds to a uniform prior between 0 and 1. The run is simulated for 1000 steps and the results at certain steps are plotted below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb89c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plotting functions\n",
    "# Iterations to plot\n",
    "plots = [1, 2, 5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "\n",
    "def plot(priors, step, ax):\n",
    "    \"\"\"Plot the priors for the current step.\"\"\"\n",
    "    plot_x = np.linspace(0.001, .999, 100)\n",
    "    for prior in priors:\n",
    "        y = prior.pdf(plot_x)\n",
    "        p = ax.plot(plot_x, y)\n",
    "        ax.fill_between(plot_x, y, 0, alpha=0.2)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim(bottom=0)\n",
    "    ax.set_title(f'Priors at step {step:d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c86c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate multi-armed bandit process and update posteriors\n",
    "\n",
    "# Setup plot\n",
    "fig, axs = plt.subplots(5, 2, figsize=(8, 10))\n",
    "axs = axs.flat\n",
    "\n",
    "# The number of trials and wins will represent the prior for each\n",
    "#  bandit with the help of the Beta distribution.\n",
    "trials = [0, 0, 0]  # Number of times we tried each bandit\n",
    "wins = [0, 0, 0]  # Number of wins for each bandit\n",
    "\n",
    "n = 1000\n",
    "# Run the trail for `n` steps\n",
    "for step in range(1, n+1):\n",
    "    # Define the prior based on current observations\n",
    "    bandit_priors = [\n",
    "        stats.beta(a=1+w, b=1+t-w) for t, w in zip(trials, wins)]\n",
    "    # plot prior \n",
    "    if step in plots:\n",
    "        plot(bandit_priors, step, next(axs))\n",
    "    # Sample a probability theta for each bandit\n",
    "    theta_samples = [\n",
    "        d.rvs(1) for d in bandit_priors\n",
    "    ]\n",
    "    # choose a bandit\n",
    "    chosen_bandit = np.argmax(theta_samples)\n",
    "    # Pull the bandit\n",
    "    x = pull(chosen_bandit)\n",
    "    # Update trials and wins (defines the posterior)\n",
    "    trials[chosen_bandit] += 1\n",
    "    wins[chosen_bandit] += x\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd61f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final outcome and number of test needed per bandit\n",
    "emperical_p_bandits = [(1+w) / (1+t) for t, w in zip(trials, wins)]\n",
    "for i in range(nb_bandits):\n",
    "    print((f'True prob={p_bandits[i]:.2f};  '\n",
    "           f'Emperical prob={emperical_p_bandits[i]:.2f};  '\n",
    "           f'Trials={trials[i]:d}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76729150",
   "metadata": {},
   "source": [
    "Note above that the algorithm quickly converges to select the bandit with the highest probability of winning. After 1000 iterations it has chosen the winning bandit 10 times more often than one of the other bandits.\n",
    "\n",
    "Again, this case study was entirely optional, and here only for you to explore more advanced topics around MAB (you will not be asked any homework question on this case study). Read the material at the following links to dive deeper: <br>\n",
    "* https://towardsdatascience.com/thompson-sampling-fc28817eacb8 <br>\n",
    "* https://peterroelants.github.io/posts/multi-armed-bandit-implementation/#Further-readings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2667d14",
   "metadata": {},
   "source": [
    "## Thank you everyone!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
