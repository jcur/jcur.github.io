{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 8\n",
    "## Fundamental Actor-Critic Algorithms\n",
    "\n",
    "#### Professor - Jeremy Curuksu, PhD (jeremy.cur@nyu.edu)\n",
    "\n",
    "#### Section Leader - Anudeep Tubati (at5373@nyu.edu)\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "In today's lab, we will learn to implement two essential algorithms from the Actor-Critic family: Advantage Actor Critic (https://openai.com/research/openai-baselines-acktr-a2c), and Deep Deterministic Policy Gradients (https://www.deepmind.com/publications/deterministic-policy-gradient-algorithms).\n",
    "\n",
    "<br>\n",
    "\n",
    "A2C helps in reducing the variance of the updates stemming from the empirical cumulative reward as seen in REINFORCE. We will see how it does so. Whereas, DDPG focuses on a different kind of problems, those with continuous action spaces. Can you think of a way to solve continuous action spaces using DQNs? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "### Advantage Actor Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recollect the policy gradient update from REINFORCE: $\\sum_t \\nabla G_t \\log \\pi(a_t | s_t)$\n",
    "\n",
    "#### Using Q(s, a) and V(s)\n",
    "Let's compute the advantage of an action defined by\n",
    "\n",
    "<center>$A(s, a) = Q(s, a) - V(s)$</center>\n",
    "\n",
    "Note: We may also subtract the average return of state s instead of using V(s). It is called using a baseline. However, it is easy to see that approximating V(s) non-linearly may be better than using a moving average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantage Actor-Critic Algorithm on Pong, Author: Anudeep Tubati, NYU\n",
    "# Modified implementation of REINFORCE at https://karpathy.github.io/2016/05/31/rl/\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Bernoulli\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# code for the only two actions in Pong\n",
    "# 2 -> UP, 3 -> DOWN\n",
    "ACTIONS = [2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(img):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6000 (75x80) 1D float vector \"\"\"\n",
    "    img = img[35:185]\n",
    "    img = img[::2, ::2, 0]\n",
    "    img[img == 144] = 0\n",
    "    img[img == 109] = 0\n",
    "    img[img != 0] = 1\n",
    "\n",
    "    return img.astype(np.float32).ravel()\n",
    "\n",
    "def discount_rewards(reward):\n",
    "    # Compute the gamma-discounted rewards over an episode\n",
    "    gamma = 0.99    # discount rate\n",
    "    running_add = 0\n",
    "    discounted_r = torch.zeros_like(reward)\n",
    "\n",
    "    for i in reversed(range(0, len(reward))):\n",
    "        if reward[i] != 0: # reset the sum, since this was a game boundary (pong specific!)\n",
    "            running_add = 0\n",
    "        running_add = running_add * gamma + reward[i]\n",
    "        discounted_r[i] = running_add\n",
    "\n",
    "    discounted_r -= torch.mean(discounted_r) # normalizing the result\n",
    "    discounted_r /= torch.std(discounted_r) # divide by standard deviation\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "def log(filename, string):\n",
    "    with open(filename, 'a+') as logger:\n",
    "        logger.write(string)\n",
    "\n",
    "\n",
    "# wrapper for the Gym environment that uses our helper functions\n",
    "class Pong(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "\n",
    "        self.ROWS = 75\n",
    "        self.COLS = 80\n",
    "\n",
    "        self.state_size = (self.ROWS * self.COLS, )\n",
    "\n",
    "    def reset(self):\n",
    "        self.prev_x = prepro(self.env.reset()[0])\n",
    "\n",
    "        return np.zeros(self.state_size).flatten()\n",
    "    \n",
    "    def step(self, action):\n",
    "        next_state, reward, done, _, info = self.env.step(action)\n",
    "\n",
    "        next_state = prepro(next_state)\n",
    "        actual_next_state = next_state - self.prev_x\n",
    "        self.prev_x = next_state\n",
    "\n",
    "        return actual_next_state.flatten(), reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent and Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, 200)\n",
    "        self.output = nn.Linear(200, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        action_prob = torch.sigmoid(self.output(x))\n",
    "\n",
    "        return action_prob\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, 200)\n",
    "        self.output = nn.Linear(200, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        value = self.output(x)\n",
    "\n",
    "        return value\n",
    "\n",
    "\n",
    "class A2CAgent(object):\n",
    "    def __init__(self, input_size, log_filename):\n",
    "        self.actor = Actor(input_size, 1)\n",
    "        self.critic = Critic(input_size)\n",
    "        self.optimizerA = optim.RMSprop(self.actor.parameters(), lr=0.001, weight_decay=0.99)\n",
    "        self.optimizerC = optim.RMSprop(self.critic.parameters(), lr=0.001, weight_decay=0.99)\n",
    "\n",
    "        self.memory = {\n",
    "            'rewards': [],\n",
    "            'log_probs': [],\n",
    "            'states': []\n",
    "        }\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.log_filename = log_filename\n",
    "    \n",
    "    def select_action(self, state):\n",
    "\n",
    "        # sample an action from stochastic policy\n",
    "        action_prob = self.actor.forward(state)\n",
    "        dist = Bernoulli(action_prob)\n",
    "\n",
    "        sampled_val = dist.sample()\n",
    "        action_idx = int(sampled_val.item())\n",
    "\n",
    "        # compute log prob\n",
    "        # print(sampled_val.item() == 1.0, sampled_val, action_idx)\n",
    "        action_to_take = ACTIONS[action_idx]\n",
    "\n",
    "        self.memory['log_probs'].append(dist.log_prob(sampled_val))\n",
    "\n",
    "        return action_to_take\n",
    "\n",
    "    def remember(self, state, reward):\n",
    "        self.memory['states'].append(state)\n",
    "        self.memory['rewards'].append(reward)\n",
    "    \n",
    "    def update_network(self):\n",
    "        len_r = len(self.memory['rewards'])\n",
    "        assert len_r == len(self.memory['log_probs'])\n",
    "\n",
    "        # convert to tensors for ease of operation\n",
    "        self.memory['rewards'] = torch.tensor(self.memory['rewards'], dtype=torch.float32)\n",
    "        discounted_r = discount_rewards(self.memory['rewards']).unsqueeze(1)\n",
    "        self.memory['log_probs'] = torch.stack(self.memory['log_probs'])\n",
    "        states = torch.stack(self.memory['states'])\n",
    "\n",
    "        # get V values of states from critic\n",
    "        values = self.critic.forward(states)\n",
    "\n",
    "        CHANGE_SCHED_EPOCH = 1000\n",
    "\n",
    "        # train only critic first, then train policy as well\n",
    "        if self.epoch <= CHANGE_SCHED_EPOCH:\n",
    "            # calculate policy loss\n",
    "            policy_losses = (-1 * self.memory['log_probs']) * discounted_r\n",
    "\n",
    "            # calculate loss for critic\n",
    "            value_loss = F.mse_loss(values, discounted_r)\n",
    "\n",
    "        else:\n",
    "            if self.epoch == (CHANGE_SCHED_EPOCH + 9):\n",
    "                self.optimizerA.param_groups[0]['lr'] = 0.0005\n",
    "                print(\"\\nACTOR LR CHANGED TO 0.0005\")\n",
    "\n",
    "            # calculate advantages for A2C\n",
    "            advantages = discounted_r - values.detach()\n",
    "\n",
    "            # calculate policy loss\n",
    "            policy_losses = (-1 * self.memory['log_probs']) * advantages\n",
    "\n",
    "            # calculate targets for critic by adding discounted next_state\n",
    "            # values (except for last state)\n",
    "            targets = self.memory['rewards'].unsqueeze(1).clone()\n",
    "            targets[:-1] += (0.99 * values.detach())[1:]\n",
    "\n",
    "            # calculate value loss from targets\n",
    "            value_loss = F.mse_loss(values, targets)\n",
    "\n",
    "        # print(f\"[{self.epoch}]\", value_loss)\n",
    "\n",
    "        # crux of training\n",
    "        self.optimizerA.zero_grad()\n",
    "        self.lossA = policy_losses.sum()\n",
    "        self.lossA.backward()\n",
    "        self.optimizerA.step()\n",
    "\n",
    "        self.optimizerC.zero_grad()\n",
    "        self.lossC = value_loss\n",
    "        self.lossC.backward()\n",
    "        self.optimizerC.step()\n",
    "\n",
    "        # reset memory because this is on-policy\n",
    "        for k in self.memory.keys():\n",
    "            self.memory[k] = []\n",
    "    \n",
    "    def learn(self, env, num_epochs, roll_size, start=0):\n",
    "\n",
    "        print(f\"Resuming from {start + 1}, Writing to {self.log_filename}\\n\")\n",
    "        # self.log_file.write(f\"Resuming from {start + 1}\\n\\n\")\n",
    "\n",
    "        assert roll_size == 10\n",
    "\n",
    "        avg = -float('inf')\n",
    "        best_avg = -float('inf')\n",
    "        max_score = -float('inf')\n",
    "        all_scores = np.zeros((num_epochs, ), dtype=np.int32)\n",
    "\n",
    "        for eps_idx in range(start + 1, num_epochs):\n",
    "            self.epoch = eps_idx\n",
    "\n",
    "            # beginning of an episode\n",
    "            state = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            done = False\n",
    "            score = 0\n",
    "\n",
    "            while not done:\n",
    "\n",
    "                action = self.select_action(state)\n",
    "\n",
    "                # run one step\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "                self.remember(state, reward)\n",
    "                state = next_state\n",
    "\n",
    "                score += reward\n",
    "\n",
    "            # bookkeeping of stats\n",
    "            all_scores[eps_idx] = score\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "\n",
    "            sys.stdout.write(f\"\\r [{eps_idx}]: {score}, Avg: {avg:.2f}, Max: {max_score}, Best_avg: {best_avg:.2f}\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            if ((eps_idx + 1) % roll_size) == 0:\n",
    "                avg = np.mean(all_scores[(eps_idx + 1) - roll_size:eps_idx])\n",
    "                if avg > best_avg:\n",
    "                    best_avg = avg\n",
    "                    self.save(eps_idx, \"pong_checkpoint_bestavg\")\n",
    "\n",
    "                # print(f\"\\n [{eps_idx}]: {score}, Avg: {avg:.2f}, Max: {max_score}, Best_avg: {best_avg:.2f}\")\n",
    "                stat_string = f\" [{eps_idx}]: {score}, Avg: {avg:.2f}, Max: {max_score}, Best_avg: {best_avg:.2f}\\n\"\n",
    "                log(self.log_filename, stat_string)\n",
    "                self.save(eps_idx, \"pong_checkpoint_latest\")\n",
    "\n",
    "                np.save('checkpoints/all_scores.npy', all_scores, allow_pickle=False)\n",
    "            \n",
    "            # train every 10 episodes\n",
    "            if ((eps_idx + 1) % 10) == 0:\n",
    "                self.update_network()\n",
    "            \n",
    "            # graph the scores every 100 eps\n",
    "            if ((eps_idx + 1) % 100) == 0:\n",
    "                pass\n",
    "        \n",
    "        avg = np.mean(all_scores)\n",
    "        max_score = np.max(all_scores)\n",
    "        print(f\"\\n [{eps_idx}]: {score}, Avg: {avg:.2f}, Max: {max_score}, Best_avg: {best_avg:.2f}\")\n",
    "\n",
    "    def save(self, epoch, path):\n",
    "        save_dir = 'checkpoints/'\n",
    "        path = save_dir + path + \".pt\"\n",
    "\n",
    "        Path(save_dir).mkdir(exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            lossA = self.lossA\n",
    "            lossC = self.lossC\n",
    "        except AttributeError:\n",
    "            lossA = None\n",
    "            lossC = None\n",
    "\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'actor_state_dict': self.actor.state_dict(),\n",
    "                'critic_state_dict': self.critic.state_dict(),\n",
    "                'optimizerA_state_dict': self.optimizerA.state_dict(),\n",
    "                'optimizerC_state_dict': self.optimizerC.state_dict(),\n",
    "                'lossA': lossA,\n",
    "                'lossC': lossC,\n",
    "            },\n",
    "            path\n",
    "        )\n",
    "\n",
    "    def load(self, path):\n",
    "        save_dir = 'checkpoints/'\n",
    "        path = save_dir + path + \".pt\"\n",
    "\n",
    "        checkpoint = torch.load(path)\n",
    "\n",
    "        epoch = checkpoint['epoch']\n",
    "        self.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "        self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "        self.optimizerA.load_state_dict(checkpoint['optimizerA_state_dict'])\n",
    "        self.optimizerC.load_state_dict(checkpoint['optimizerC_state_dict'])\n",
    "\n",
    "        return epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from 0, Writing to logs.txt\n",
      "\n",
      " [101]: -21.0, Avg: -20.33, Max: -16.0, Best_avg: -20.00"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"PongDeterministic-v0\")\n",
    "env = Pong(env)\n",
    "\n",
    "if Path('logs.txt').exists():\n",
    "    print(\"Logs already exist, appending to them.\")\n",
    "agent = A2CAgent(6000, 'logs.txt')\n",
    "epoch_resume = -1\n",
    "# epoch_resume = agent.load('pong_checkpoint_bestavg')\n",
    "agent.learn(env, 1000, 10, epoch_resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II\n",
    "### Deep Deterministic Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# derived from https://github.com/wpiszlogin/driver_critic/\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tools import *\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Class for the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike DQN where we set target network weights to the trainable network weights in defined intervals, we use a smoother update for the target networks' weights. \n",
    "![tau_update](https://spinningup.openai.com/en/latest/_images/math/d417987803ca9f61ac60741880a748129bd66dde.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSolution:\n",
    "    def __init__(self, action_space, model_outputs=None, noise_mean=None, noise_std=None):\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.gamma = 0.99\n",
    "        self.actor_lr = 0.00001\n",
    "        self.critic_lr = 0.002\n",
    "        self.tau = 0.005\n",
    "        self.memory_capacity = 60000\n",
    "\n",
    "        # For problems that have specific outputs of an actor model\n",
    "        self.need_decode_out = model_outputs is not None\n",
    "        self.model_action_out = model_outputs if model_outputs else action_space.shape[0]\n",
    "        self.action_space = action_space\n",
    "\n",
    "        # Init noise generator\n",
    "        if noise_mean is None:\n",
    "            noise_mean = np.full(self.model_action_out, 0.0, np.float32)\n",
    "        if noise_std is None:\n",
    "            noise_std  = np.full(self.model_action_out, 0.2, np.float32)\n",
    "        std = self.noise = NoiseGenerator(noise_mean, noise_std)\n",
    "\n",
    "        # Initialize buffer R\n",
    "        self.r_buffer = MemoriesRecorder(memory_capacity=self.memory_capacity)\n",
    "\n",
    "        self.actor_opt      = Adam(self.actor_lr)\n",
    "        self.critic_opt     = Adam(self.critic_lr)\n",
    "        self.actor          = None\n",
    "        self.critic         = None\n",
    "        self.target_actor   = None\n",
    "        self.target_critic  = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def build_actor(self, state_shape, name=\"Actor\"):\n",
    "        inputs = layers.Input(shape=state_shape)\n",
    "        x = inputs\n",
    "        x = layers.Conv2D(16, kernel_size=(5, 5), strides=(4, 4), padding='valid', use_bias=False, activation=\"relu\")(x)\n",
    "        x = layers.Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='valid', use_bias=False, activation=\"relu\")(x)\n",
    "        x = layers.Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='valid', use_bias=False, activation=\"relu\")(x)\n",
    "\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        y = layers.Dense(self.model_action_out, activation='tanh')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=y, name=name)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def build_critic(self, state_shape, name=\"Critic\"):\n",
    "        state_inputs = layers.Input(shape=state_shape)\n",
    "        x = state_inputs\n",
    "        x = layers.Conv2D(16, kernel_size=(5, 5), strides=(4, 4), padding='valid', use_bias=False, activation=\"relu\")(x)\n",
    "        x = layers.Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='valid', use_bias=False, activation=\"relu\")(x)\n",
    "        x = layers.Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='valid', use_bias=False, activation=\"relu\")(x)\n",
    "\n",
    "        x = layers.Flatten()(x)\n",
    "        action_inputs = layers.Input(shape=(self.model_action_out,))\n",
    "        x = layers.concatenate([x, action_inputs])\n",
    "\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        x = layers.Dense(32, activation='relu')(x)\n",
    "        y = layers.Dense(1)(x)\n",
    "\n",
    "        model = Model(inputs=[state_inputs, action_inputs], outputs=y, name=name)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def init_networks(self, state_shape):\n",
    "        self.actor  = self.build_actor(state_shape)\n",
    "        self.critic = self.build_critic(state_shape)\n",
    "\n",
    "        # Build target networks in the same way\n",
    "        self.target_actor  = self.build_actor(state_shape, name='TargetActor')\n",
    "        self.target_critic = self.build_critic(state_shape, name='TargetCritic')\n",
    "\n",
    "        # Copy parameters from action and critic\n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "\n",
    "    def get_action(self, state, add_noise=True):\n",
    "        prep_state = self.preprocess(state)\n",
    "        if self.actor is None:\n",
    "            self.init_networks(prep_state.shape)\n",
    "\n",
    "        # Get result from a network\n",
    "        tensor_state = tf.expand_dims(tf.convert_to_tensor(prep_state), 0)\n",
    "        actor_output = self.actor(tensor_state).numpy()\n",
    "\n",
    "        # Add noise\n",
    "        if add_noise:\n",
    "            actor_output = actor_output[0] + self.noise.generate()\n",
    "        else:\n",
    "            actor_output = actor_output[0]\n",
    "\n",
    "        if self.need_decode_out:\n",
    "            env_action = self.decode_model_output(actor_output)\n",
    "        else:\n",
    "            env_action = actor_output\n",
    "\n",
    "        # Clip min-max\n",
    "        env_action = np.clip(np.array(env_action), a_min=self.action_space.low, a_max=self.action_space.high)\n",
    "        return env_action, actor_output\n",
    "\n",
    "    def decode_model_output(self, model_out):\n",
    "        return np.array([model_out[0], model_out[1].clip(0, 1), -model_out[1].clip(-1, 0)])\n",
    "\n",
    "    def preprocess(self, img, greyscale=False):\n",
    "        img = img.copy()\n",
    "        # Remove numbers and enlarge speed bar\n",
    "        for i in range(88, 93+1):\n",
    "            img[i, 0:12, :] = img[i, 12, :]\n",
    "\n",
    "        # Unify grass color\n",
    "        replace_color(img, original=(102, 229, 102), new_value=(102, 204, 102))\n",
    "\n",
    "        if greyscale:\n",
    "            img = img.mean(axis=2)\n",
    "            img = np.expand_dims(img, 2)\n",
    "\n",
    "        # Make car black\n",
    "        car_color = 68.0\n",
    "        car_area = img[67:77, 42:53]\n",
    "        car_area[car_area == car_color] = 0\n",
    "\n",
    "        # Scale from 0 to 1\n",
    "        img = img / img.max()\n",
    "\n",
    "        # Unify track color\n",
    "        img[(img > 0.411) & (img < 0.412)] = 0.4\n",
    "        img[(img > 0.419) & (img < 0.420)] = 0.4\n",
    "\n",
    "        # Change color of kerbs\n",
    "        game_screen = img[0:83, :]\n",
    "        game_screen[game_screen == 1] = 0.80\n",
    "        return img\n",
    "\n",
    "    def learn(self, state, train_action, reward, new_state):\n",
    "        # Store transition in R\n",
    "        prep_state     = self.preprocess(state)\n",
    "        prep_new_state = self.preprocess(new_state)\n",
    "        self.r_buffer.write(prep_state, train_action, reward, prep_new_state)\n",
    "\n",
    "        # Sample mini-batch from R\n",
    "        state_batch, action_batch, reward_batch, new_state_batch  = self.r_buffer.sample()\n",
    "\n",
    "        state_batch     = tf.convert_to_tensor(state_batch)\n",
    "        action_batch    = tf.convert_to_tensor(action_batch)\n",
    "        reward_batch    = tf.convert_to_tensor(reward_batch)\n",
    "        reward_batch    = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        new_state_batch = tf.convert_to_tensor(new_state_batch)\n",
    "\n",
    "        self.update_actor_critic(state_batch, action_batch, reward_batch, new_state_batch)\n",
    "\n",
    "        # Update target networks\n",
    "        self.update_target_network(self.target_actor.variables, self.actor.variables)\n",
    "        self.update_target_network(self.target_critic.variables, self.critic.variables)\n",
    "\n",
    "    @tf.function\n",
    "    def update_actor_critic(self, state, action, reward, new_state):\n",
    "        # Update critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Calc y\n",
    "            new_action = self.target_actor(new_state, training=True)\n",
    "            y = reward + self.gamma * self.target_critic([new_state, new_action], training=True)\n",
    "\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - self.critic([state, action], training=True)))\n",
    "\n",
    "        critic_gradients = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        self.critic_opt.apply_gradients(zip(critic_gradients, self.critic.trainable_variables))\n",
    "\n",
    "        # Update actor\n",
    "        with tf.GradientTape() as tape:\n",
    "            critic_out = self.critic([state, self.actor(state, training=True)], training=True)\n",
    "            actor_loss = -tf.math.reduce_mean(critic_out)  # Need to maximize\n",
    "\n",
    "        actor_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor_opt.apply_gradients(zip(actor_gradients, self.actor.trainable_variables))\n",
    "\n",
    "    @tf.function\n",
    "    def update_target_network(self, target_weights, new_weights):\n",
    "        for t, n in zip(target_weights, new_weights):\n",
    "            t.assign((1 - self.tau) * t + self.tau * n)\n",
    "\n",
    "    def save_solution(self, path='models/'):\n",
    "        self.actor.save(path + 'actor.h5')\n",
    "        self.critic.save(path + 'critic.h5')\n",
    "        self.target_actor.save(path + 'target_actor.h5')\n",
    "        self.target_critic.save(path + 'target_critic.h5')\n",
    "\n",
    "    def load_solution(self, path='models/'):\n",
    "        self.actor = tf.keras.models.load_model(path + 'actor.h5')\n",
    "        self.critic = tf.keras.models.load_model(path + 'critic.h5')\n",
    "        self.target_actor = tf.keras.models.load_model(path + 'target_actor.h5')\n",
    "        self.target_critic = tf.keras.models.load_model(path + 'target_critic.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "Gets an average reward of 400 with 30 mins of training on an Intel i5 7th Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_episodes = 50000\n",
    "problem = 'CarRacing-v2'\n",
    "\n",
    "gym.logger.set_level(40)\n",
    "preview = False\n",
    "best_result = 0\n",
    "all_episode_reward = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize simulation\n",
    "env = gym.make(problem)\n",
    "state, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 09:44:34.602618: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/anudeep/anaconda3/envs/py39/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-03-24 09:44:34.603680: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-24 09:44:34.603720: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (anudeep-ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2023-03-24 09:44:34.810075: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Define custom standard deviation for noise\n",
    "# We need lesser noise for steering\n",
    "noise_std = np.array([0.1, 4 * 0.2], dtype=np.float32)\n",
    "solution = BaseSolution(env.action_space, model_outputs=2, noise_std=noise_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Actor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 23, 23, 16)        1200      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 32)          4608      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 2, 2, 32)          9216      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,410\n",
      "Trainable params: 23,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Critic\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 96, 96, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 23, 23, 16)   1200        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 7, 7, 32)     4608        ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 2, 2, 32)     9216        ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 128)          0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 130)          0           ['flatten_1[0][0]',              \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           8384        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 32)           2080        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            33          ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 25,521\n",
      "Trainable params: 25,521\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"TargetActor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 23, 23, 16)        1200      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 7, 7, 32)          4608      \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 2, 2, 32)          9216      \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,410\n",
      "Trainable params: 23,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"TargetCritic\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 96, 96, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 23, 23, 16)   1200        ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 7, 7, 32)     4608        ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 2, 2, 32)     9216        ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 128)          0           ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 130)          0           ['flatten_3[0][0]',              \n",
      "                                                                  'input_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 64)           8384        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 32)           2080        ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            33          ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 25,521\n",
      "Trainable params: 25,521\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Last result: 99.90147601476157 Average results: 99.90147601476157\n",
      "Saving best solution\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last result: -10.024183006535866 Average results: 44.93864650411285\n",
      "Last result: -10.224475524475443 Average results: 26.55093916125009\n",
      "Last result: 19.695857988165315 Average results: 24.837168867978896\n",
      "Last result: -18.044927536231803 Average results: 16.260749587136758\n",
      "Last result: -9.896699669966893 Average results: 11.901174710952816\n",
      "Last result: -7.322292993630489 Average results: 9.154965038869488\n",
      "Last result: -10.639335180055317 Average results: 6.680677511503886\n",
      "Last result: -18.255279503105534 Average results: 3.910015620991728\n",
      "Last result: -3.0946236559139377 Average results: 3.2095516933011616\n",
      "Last result: -1.4118644067796382 Average results: -6.921782348852962\n",
      "Last result: 1.7827160493826804 Average results: -5.741092443261106\n",
      "Last result: -11.76666666666674 Average results: -5.895311557480237\n",
      "Last result: 52.60565371024762 Average results: -2.6043319852720046\n",
      "Last result: 19.759414225942024 Average results: 1.1761021909453775\n",
      "Last result: 58.730894308943796 Average results: 8.038861588836447\n",
      "Last result: 43.02550335570431 Average results: 13.073641223769926\n",
      "Last result: 246.75751633986843 Average results: 38.8133263757623\n",
      "Saving best solution\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last result: 397.2449275362261 Average results: 80.36334707969546\n",
      "Saving best solution\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last result: 666.2965986394485 Average results: 147.30246930923172\n",
      "Saving best solution\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last result: 849.9999999999816 Average results: 232.4436557499078\n",
      "Saving best solution\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last result: 862.4573378839391 Average results: 318.51111793336344\n",
      "Saving best solution\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last result: 351.67416974169157 Average results: 354.8552015741993\n",
      "Last result: 536.3636363636225 Average results: 403.23099983953676\n",
      "Last result: 421.8978102189742 Average results: 443.44483943883995\n",
      "Last result: 634.7846416382132 Average results: 501.0502141717669\n",
      "Last result: 152.61634980988768 Average results: 512.0092988171853\n",
      "Last result: 81.5971830985928 Average results: 495.4932654930577\n",
      "Last result: 415.92811501596856 Average results: 497.36158424103195\n",
      "Last result: 475.4413249211261 Average results: 478.2760568691998\n",
      "Last result: 616.6666666666601 Average results: 454.9427235358676\n",
      "Last result: 252.98436578170808 Average results: 393.9954263256445\n",
      "Last result: 405.2586206896476 Average results: 399.3538714204401\n",
      "Last result: 446.5945054944993 Average results: 390.3769583335278\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Models action output has a different shape for this problem\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43msolution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m state \u001b[38;5;241m=\u001b[39m new_state\n\u001b[1;32m     24\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[5], line 154\u001b[0m, in \u001b[0;36mBaseSolution.learn\u001b[0;34m(self, state, train_action, reward, new_state)\u001b[0m\n\u001b[1;32m    151\u001b[0m reward_batch    \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(reward_batch, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    152\u001b[0m new_state_batch \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(new_state_batch)\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_actor_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_state_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Update target networks\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_target_network(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_actor\u001b[38;5;241m.\u001b[39mvariables, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mvariables)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop of episodes\n",
    "for ie in range(n_episodes):\n",
    "    state, info = env.reset()\n",
    "    solution.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    no_reward_counter = 0\n",
    "\n",
    "    # One-step-loop\n",
    "    while not done:\n",
    "        if preview:\n",
    "            env.render()\n",
    "\n",
    "        action, train_action = solution.get_action(state)\n",
    "\n",
    "        # This will make steering much easier\n",
    "        action /= 4\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Models action output has a different shape for this problem\n",
    "        solution.learn(state, train_action, reward, new_state)\n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if reward < 0:\n",
    "            no_reward_counter += 1\n",
    "            if no_reward_counter > 200:\n",
    "                break\n",
    "        else:\n",
    "            no_reward_counter = 0\n",
    "\n",
    "    all_episode_reward.append(episode_reward)\n",
    "    average_result = np.array(all_episode_reward[-10:]).mean()\n",
    "    print('Last result:', episode_reward, 'Average results:', average_result)\n",
    "\n",
    "    if episode_reward > best_result:\n",
    "        print('Saving best solution')\n",
    "        solution.save_solution()\n",
    "        best_result = episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 17:00:46.037058: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/anudeep/anaconda3/envs/py39/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-03-24 17:00:46.042958: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-24 17:00:46.043162: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (anudeep-ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2023-03-24 17:00:46.276822: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# We can improve stability of solution with noise\n",
    "noise_mean = np.array([0.0, -0.83], dtype=np.float32)\n",
    "noise_std = np.array([0.0, 4 * 0.02], dtype=np.float32)\n",
    "solution = BaseSolution(env.action_space, model_outputs=2, noise_mean=noise_mean, noise_std=noise_std)\n",
    "solution.load_solution('models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10\n",
    "env = gym.make(problem, render_mode=\"human\")\n",
    "state, info = env.reset()\n",
    "all_episode_reward = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anudeep/anaconda3/envs/py39/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last result: 874.3589743589607 Average results: 874.3589743589607\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# This will make steering much easier\u001b[39;00m\n\u001b[1;32m     15\u001b[0m action \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m---> 16\u001b[0m new_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m     19\u001b[0m state \u001b[38;5;241m=\u001b[39m new_state\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/gymnasium/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/gymnasium/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/gymnasium/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/gymnasium/envs/box2d/car_racing.py:545\u001b[0m, in \u001b[0;36mCarRacing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39mStep(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m FPS, \u001b[38;5;241m6\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m FPS\n\u001b[0;32m--> 545\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_pixels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m step_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    548\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/gymnasium/envs/box2d/car_racing.py:600\u001b[0m, in \u001b[0;36mCarRacing._render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    597\u001b[0m trans \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mVector2((scroll_x, scroll_y))\u001b[38;5;241m.\u001b[39mrotate_rad(angle)\n\u001b[1;32m    598\u001b[0m trans \u001b[38;5;241m=\u001b[39m (WINDOW_W \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m trans[\u001b[38;5;241m0\u001b[39m], WINDOW_H \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m+\u001b[39m trans[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 600\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_road\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzoom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcar\u001b[38;5;241m.\u001b[39mdraw(\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf,\n\u001b[1;32m    603\u001b[0m     zoom,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m     mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_pixels_list\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_pixels\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    607\u001b[0m )\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mflip(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/gymnasium/envs/box2d/car_racing.py:671\u001b[0m, in \u001b[0;36mCarRacing._render_road\u001b[0;34m(self, zoom, translation, angle)\u001b[0m\n\u001b[1;32m    669\u001b[0m poly \u001b[38;5;241m=\u001b[39m [(p[\u001b[38;5;241m0\u001b[39m], p[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m poly]\n\u001b[1;32m    670\u001b[0m color \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m color]\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_colored_polygon\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzoom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/gymnasium/envs/box2d/car_racing.py:744\u001b[0m, in \u001b[0;36mCarRacing._draw_colored_polygon\u001b[0;34m(self, surface, poly, color, zoom, translation, angle, clip)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_draw_colored_polygon\u001b[39m(\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28mself\u001b[39m, surface, poly, color, zoom, translation, angle, clip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    743\u001b[0m ):\n\u001b[0;32m--> 744\u001b[0m     poly \u001b[38;5;241m=\u001b[39m [pygame\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mVector2(c)\u001b[38;5;241m.\u001b[39mrotate_rad(angle) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m poly]\n\u001b[1;32m    745\u001b[0m     poly \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    746\u001b[0m         (c[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m zoom \u001b[38;5;241m+\u001b[39m translation[\u001b[38;5;241m0\u001b[39m], c[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m zoom \u001b[38;5;241m+\u001b[39m translation[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m poly\n\u001b[1;32m    747\u001b[0m     ]\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;66;03m# This checks if the polygon is out of bounds of the screen, and we skip drawing if so.\u001b[39;00m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;66;03m# Instead of calculating exactly if the polygon and screen overlap,\u001b[39;00m\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;66;03m# we simply check if the polygon is in a larger bounding box whose dimension\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# is greater than the screen by MAX_SHAPE_DIM, which is the maximum\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;66;03m# diagonal length of an environment object\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/gymnasium/envs/box2d/car_racing.py:744\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_draw_colored_polygon\u001b[39m(\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28mself\u001b[39m, surface, poly, color, zoom, translation, angle, clip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    743\u001b[0m ):\n\u001b[0;32m--> 744\u001b[0m     poly \u001b[38;5;241m=\u001b[39m [\u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVector2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrotate_rad(angle) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m poly]\n\u001b[1;32m    745\u001b[0m     poly \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    746\u001b[0m         (c[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m zoom \u001b[38;5;241m+\u001b[39m translation[\u001b[38;5;241m0\u001b[39m], c[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m zoom \u001b[38;5;241m+\u001b[39m translation[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m poly\n\u001b[1;32m    747\u001b[0m     ]\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;66;03m# This checks if the polygon is out of bounds of the screen, and we skip drawing if so.\u001b[39;00m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;66;03m# Instead of calculating exactly if the polygon and screen overlap,\u001b[39;00m\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;66;03m# we simply check if the polygon is in a larger bounding box whose dimension\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# is greater than the screen by MAX_SHAPE_DIM, which is the maximum\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;66;03m# diagonal length of an environment object\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop of episodes\n",
    "for ie in range(n_episodes):\n",
    "    state, info = env.reset()\n",
    "    solution.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    no_reward_counter = 0\n",
    "\n",
    "    # One-step-loop\n",
    "    while not done:\n",
    "\n",
    "        action, train_action = solution.get_action(state)\n",
    "\n",
    "        # This will make steering much easier\n",
    "        action /= 4\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if reward < 0:\n",
    "            no_reward_counter += 1\n",
    "            if no_reward_counter > 200:\n",
    "                break\n",
    "        else:\n",
    "            no_reward_counter = 0\n",
    "\n",
    "    all_episode_reward.append(episode_reward)\n",
    "    average_result = np.array(all_episode_reward).mean()\n",
    "    print('Last result:', episode_reward, 'Average results:', average_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Improve the DDPG Agent\n",
    "1) This code does not use braking efficiently. Find out how and fix it to get a better agent.\n",
    "\n",
    "2) This code does not make use of the `terminated` flag. Add it to the replay buffer and updates. Check if the model converges faster after doing so.\n",
    "\n",
    "3) Evaluate the agent without noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
