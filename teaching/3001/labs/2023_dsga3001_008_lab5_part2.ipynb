{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d1c834",
   "metadata": {},
   "source": [
    "### CDS NYU\n",
    "### DS-GA 3001 | Reinforcement Learning\n",
    "### Lab 05\n",
    "### March 01, 2023\n",
    "\n",
    "\n",
    "# Deep Q-learning algorithm (from scratch...)\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267de1c0",
   "metadata": {},
   "source": [
    "## Professor\n",
    "\n",
    "\n",
    "Jeremy Curuksu, PhD -- jeremy.cur@nyu.edu\n",
    "\n",
    "## Section Leader\n",
    "\n",
    "\n",
    "Anudeep Tubati -- at5373@nyu.edu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c5acca",
   "metadata": {},
   "source": [
    "## Goal of Today's Lab \n",
    "\n",
    "In this lab, we will create a Deep Reinforcement Learning method called Deep-Q-Network (DQN). We will again use a simple environment from OpenAI Gym, but you will showcase the enormous gain we get by switching from tabular Q-Learning to Deep Q Learning.\n",
    "\n",
    "## Resources\n",
    "\n",
    "* https://gymnasium.farama.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71feef79",
   "metadata": {},
   "source": [
    "#### New Env Installation\n",
    "We need to create a new environment (just 3 packages) for this lab as it has breaking dependencies compared to our usual environment.\n",
    "\n",
    "1. `conda create --name py39_fl_bird python=3.9 -y`\n",
    "2. `conda activate py39_fl_bird`\n",
    "3. `python -m pip install --upgrade pip`\n",
    "4. `pip install flappy-bird-gym`\n",
    "5. `pip install tensorflow==2.7`\n",
    "6. `pip install ipykernel`\n",
    "7. `python -m ipykernel install --user --name=py39_fl_bird`\n",
    "\n",
    "#### Make sure to execute this notebook in the newly installed kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554f63ed",
   "metadata": {},
   "source": [
    "# 2. Solve *Flappy Bird* with DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c086c",
   "metadata": {},
   "source": [
    "In this use case, we will implement DQN to play the video game `FlappyBird`, using a different, \"*Object Oriented Programming*\" style.\n",
    "\n",
    "Link to doc of the game: https://pypi.org/project/flappy-bird-gym/\n",
    "\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time \n",
    "import numpy as np\n",
    "import flappy_bird_gym\n",
    "from collections import deque\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import load_model, save_model, Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad7840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CartPole Gym environment with graphical rendering to vizualize the environment\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "\n",
    "env.action_space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc0802",
   "metadata": {},
   "source": [
    "## Execute random actions just to get familiar with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to initial state\n",
    "env.reset()  \n",
    "\n",
    "# Loop over 1000 steps \n",
    "for i in range(1000):\n",
    "    env.render()                   # Render on the screen\n",
    "    action = 0                     # Action \"Don't jump\"\n",
    "    if i%19 == 0: action = 1       # Action \"Jump\" (1/19 lets the bird live a few sec)\n",
    "    new_state, reward, done, info = env.step(action)   # Carry out the action\n",
    "    \n",
    "    if done:\n",
    "         env.reset()\n",
    "            \n",
    "    time.sleep(0.05)  \n",
    "            \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2162f7fd",
   "metadata": {},
   "source": [
    "## Define a function that implement an Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c9898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralNetwork(input_shape, output_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=input_shape, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(output_shape, activation='linear', kernel_initializer='he_uniform'))\n",
    "    model.compile(loss='mse', optimizer=RMSprop(lr=0.0001, rho=0.95, epsilon=0.01), metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaa7b81",
   "metadata": {},
   "source": [
    "## Define a class that implement the DQN agents\n",
    "\n",
    "In this OOP style of implementation, we will define an agent class which contains:\n",
    "* Gym environment parameters\n",
    "* DQN Hyperparameters \n",
    "* Method to take actions (actor)\n",
    "* Method to learn and update DQN parameters\n",
    "* Method to train the DQN by interacting with the environment\n",
    "* Method to vizualize the trained agent behavior in test simulations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a4238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self):    \n",
    "        # Gym. environment variables\n",
    "        self.env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "        self.episodes = 1000\n",
    "        self.state_space = self.env.observation_space.shape[0]\n",
    "        self.action_space = self.env.action_space.n\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_number = 64 #16, 32, 128, 256\n",
    "        \n",
    "        self.train_start = 1000\n",
    "        self.jump_prob = 0.01\n",
    "        self.model = NeuralNetwork(input_shape=(self.state_space,), output_shape=self.action_space)\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "        return 1 if np.random.random() < self.jump_prob else 0\n",
    "\n",
    "    def learn(self):\n",
    "        # Make sure we have enough data\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "\n",
    "        # Create minibatch\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_number))\n",
    "        \n",
    "        # Variables to store minibatch info\n",
    "        state = np.zeros((self.batch_number, self.state_space))\n",
    "        next_state = np.zeros((self.batch_number, self.state_space))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        # Store data in variables\n",
    "        for i in range(self.batch_number):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])\n",
    "\n",
    "        # Predict y label\n",
    "        target = self.model.predict(state)\n",
    "        target_next = self.model.predict(next_state)\n",
    "\n",
    "        for i in range(self.batch_number):\n",
    "            if done[i]:\n",
    "                # NOTE: fill in\n",
    "            else:\n",
    "                # NOTE: fill in\n",
    "\n",
    "        self.model.fit(state, target, batch_size=self.batch_number, verbose=0)\n",
    "\n",
    "        \n",
    "    def train(self, vizualization=True):\n",
    "        \n",
    "        best_score = float('-inf')\n",
    "        # n episode Iterations for training\n",
    "        for i in range(self.episodes):\n",
    "            # Environment variables for training \n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_space])\n",
    "            done = False\n",
    "            score = 0\n",
    "            self.epsilon = self.epsilon * self.epsilon_decay if self.epsilon * self.epsilon_decay > self.epsilon_min else self.epsilon_min\n",
    "\n",
    "            while not done:\n",
    "                if vizualization: self.env.render()\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "\n",
    "                # Reshape next state\n",
    "                next_state = np.reshape(next_state, [1, self.state_space])\n",
    "                score += 1\n",
    "\n",
    "                if done:\n",
    "                    reward -= 100\n",
    "\n",
    "                self.memory.append((state, action, reward, next_state, done))\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    \n",
    "                    if i%5 == 0: print('Episode: {}\\nScore: {}\\nEpsilon: {:.2}'.format(i, score, self.epsilon))\n",
    "                    \n",
    "                    # Save model if it beats current high score\n",
    "                    if score > best_score: \n",
    "                        self.model.save('flappybrain.h5')\n",
    "                        print(f\"Great score! {score}\")\n",
    "                        best_score = score\n",
    "                        \n",
    "                        if score > 200:  # stop learning if we score high enough\n",
    "                            return\n",
    "\n",
    "                self.learn()\n",
    "\n",
    "    # Visualize a pre-trained model in test simulations\n",
    "    def perform(self):\n",
    "        self.model = load_model('flappybrain.h5')\n",
    "        while 1:\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_space])\n",
    "            done = False\n",
    "            score = 0\n",
    "\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                state = np.reshape(next_state, [1, self.state_space])\n",
    "                score += 1\n",
    "\n",
    "                print(\"Current Score: {}\".format(score))\n",
    "\n",
    "                if done: \n",
    "                    print('DEAD')\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd15b8",
   "metadata": {},
   "source": [
    "## Train the agent, or vizualize a pre-trained agent in test simulations\n",
    "\n",
    "With this OOP style implementation, we can invoke either the `train` method, or the `perform` method on a pre-trained agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58369ddc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    agent = DQNAgent()\n",
    "    agent.train(vizualization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c66f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    agent = DQNAgent()\n",
    "    agent.perform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b398b9a",
   "metadata": {},
   "source": [
    "# Exercise: \n",
    "\n",
    "## Implement a similar DQN algorithm to play at the `SuperMarioBros-v0` game.\n",
    "\n",
    "Link to doc of the game: https://pypi.org/project/gym-super-mario-bros/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a69b5",
   "metadata": {},
   "source": [
    "### Super Mario Bros setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde3a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "\n",
    "# Packages needed for Super Mario Bros\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Set up Super Mario Bros \n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, RIGHT_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68944a3",
   "metadata": {},
   "source": [
    "### Execute random actions just to get familiar with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c36a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0\n",
    "done = True\n",
    "\n",
    "for step in range(1000):\n",
    "    env.render()\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    print(info)\n",
    "    total_reward += reward\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    time.sleep(0.01) \n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c2679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your custom DQN class here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf07640",
   "metadata": {},
   "source": [
    "## Example solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a294995c",
   "metadata": {},
   "source": [
    "### Define a class that implement a DQN agent, with a CNN to process pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf18cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Create variables for our agent\n",
    "        self.state_space = state_size\n",
    "        self.action_space = action_size\n",
    "        self.memory = deque(maxlen=5000)\n",
    "        self.gamma = 0.8\n",
    "        self.chosenAction = 0\n",
    "        \n",
    "        # Exploration vs explotation\n",
    "        self.epsilon = 0.1\n",
    "        self.max_epsilon = 1\n",
    "        self.min_epsilon = 0.01\n",
    "        self.decay_epsilon = 0.0001\n",
    "        \n",
    "        # Build Neural Networks for Agent\n",
    "        self.main_network = self.build_network()\n",
    "        self.target_network = self.build_network()\n",
    "        self.update_target_network()\n",
    "        \n",
    "        \n",
    "    def build_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (4,4), strides=4, padding='same', input_shape=self.state_space))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(64, (4,4), strides=2, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(64, (3,3), strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(self.action_space, activation='linear'))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam())\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "        \n",
    "    def act(self, state, onGround):\n",
    "        if onGround < 83:\n",
    "            # print(\"On Ground\")\n",
    "            if random.uniform(0,1) < self.epsilon:\n",
    "                self.chosenAction = np.random.randint(self.action_space)\n",
    "                return self.chosenAction\n",
    "            Q_value = self.main_network.predict(state)\n",
    "            self.chosenAction = np.argmax(Q_value[0])\n",
    "            return self.chosenAction\n",
    "        else: \n",
    "            # print(\"Not on Ground\")\n",
    "            return self.chosenAction\n",
    "        \n",
    "    def update_epsilon(self, episode):\n",
    "        self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * np.exp(-self.decay_epsilon * episode)\n",
    "        \n",
    "    # Train the network\n",
    "    def train(self, batch_size):\n",
    "        #minibatch from memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        #Get variables from batch so we can find q-value\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.main_network.predict(state)\n",
    "            print(target)\n",
    "            \n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                target[0][action] = (reward + self.gamma * np.amax(self.target_network.predict(next_state)))\n",
    "                \n",
    "            self.main_network.fit(state, target, epochs=1, verbose=0)\n",
    "        \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def get_pred_act(self, state):\n",
    "        Q_values = self.main_network.predict(state)\n",
    "        print(Q_values)\n",
    "        return np.argmax(Q_values[0])\n",
    "        \n",
    "    def load(self, name):\n",
    "        self.main_network = load_model(name)\n",
    "        self.target_network = load_model(name)\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.main_network.save(name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e91dc",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000000\n",
    "num_timesteps = 400000\n",
    "batch_size = 64\n",
    "DEBUG_LENGTH = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e428be",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = env.action_space.n\n",
    "state_space = (80, 88, 1)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_state(state):\n",
    "    image = Image.fromarray(state)\n",
    "    image = image.resize((88, 80))\n",
    "    image = image.convert('L')\n",
    "    image = np.array(image)\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb56683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(state_space, action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e152ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('STARTING TRAINING')\n",
    "\n",
    "stuck_buffer = deque(maxlen=DEBUG_LENGTH)\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    Return = 0\n",
    "    done = False\n",
    "    time_step = 0\n",
    "    onGround = 79\n",
    "    \n",
    "    state = preprocess_state(env.reset())\n",
    "    state = state.reshape(-1, 80, 88, 1)\n",
    "    \n",
    "    for t in range(num_timesteps):\n",
    "        env.render()\n",
    "        time_step += 1\n",
    "        \n",
    "        if t> 1 and stuck_buffer.count(stuck_buffer[-1]) > DEBUG_LENGTH - 50:\n",
    "            action = dqn.act(state, onGround=79)\n",
    "        else:\n",
    "            action = dqn.act(state, onGround)\n",
    "        \n",
    "        print(\"ACTION IS\"+str(action))\n",
    "        \n",
    "        next_state, reward, done, info =env.step(action)\n",
    "        \n",
    "        onGround = info['y_pos']\n",
    "        stuck_buffer.append(info['x_pos'])\n",
    "        \n",
    "        next_state = preprocess_state(next_state)\n",
    "        next_state = next_state.reshape(-1, 80, 88, 1)\n",
    "        \n",
    "        dqn.store_transition(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        Return += reward\n",
    "        print(\"Episode is: {}\\nTotal Time Step: {}\\nCurrent Reward: {}\\nEpsilon is: {}\".format(str(i), str(time_step), str(Return), str(dqn.epsilon)))\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        if len(dqn.memory) > batch_size and i > 0:\n",
    "            dqn.train(batch_size)\n",
    "            \n",
    "    dqn.update_epsilon(i)\n",
    "    clear_output(wait=True)\n",
    "    dqn.update_target_network()\n",
    "\n",
    "    dqn.save('marioRL.h5')\n",
    "    \n",
    "env.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c4e1e",
   "metadata": {},
   "source": [
    "### Vizualize the trained agent in test simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daeeaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load('MarioRLmediumtrain.h5')\n",
    "\n",
    "while 1: \n",
    "    done = False\n",
    "    state = preprocess_state(env.reset())\n",
    "    state = state.reshape(-1, 80, 88, 1)\n",
    "    total_reward = 0\n",
    "    onGround = 79\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = dqn.act(state, onGround)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        onGround = info['y_pos']\n",
    "        \n",
    "        next_state = preprocess_state(next_state)\n",
    "        next_state = next_state.reshape(-1, 80, 88, 1)\n",
    "        state = next_state\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e4dcd",
   "metadata": {},
   "source": [
    "As next step, one could try to 1) train more, 2) test different hyperparameters, 3) check out different/more sophisticated implementations (there exits several papers and blog posts on \"RL for Super Mario Bros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb0cf46",
   "metadata": {},
   "source": [
    "## Thank you everyone!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_fl_bird",
   "language": "python",
   "name": "py39_fl_bird"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
